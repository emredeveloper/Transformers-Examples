{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PretrainedConfig, PreTrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siglip2VisionConfig(PretrainedConfig):\n",
    "    model_type = \"siglip2_vision\"\n",
    "    def __init__(self, hidden_size=768, intermediate_size=3072, num_hidden_layers=12, \n",
    "                 num_attention_heads=12, image_size=224, patch_size=16, num_channels=3, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_channels = num_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siglip2TextConfig(PretrainedConfig):\n",
    "    model_type = \"siglip2_text\"\n",
    "    def __init__(self, vocab_size=32000, hidden_size=768, intermediate_size=3072, \n",
    "                 num_hidden_layers=12, num_attention_heads=12, max_length=64, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.max_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siglip2Config(PretrainedConfig):\n",
    "    model_type = \"siglip2\"\n",
    "    def __init__(self, vision_config=None, text_config=None, projection_dim=768, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if vision_config is None:\n",
    "            vision_config = Siglip2VisionConfig()\n",
    "        if text_config is None:\n",
    "            text_config = Siglip2TextConfig()\n",
    "        self.vision_config = vision_config\n",
    "        self.text_config = text_config\n",
    "        self.projection_dim = projection_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # Define query with shape [1, 1, hidden_size] for proper broadcasting\n",
    "        self.query = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        # MultiheadAttention with batch_first=True for [batch_size, seq_len, hidden_size]\n",
    "        self.attn = nn.MultiheadAttention(hidden_size, num_heads=1, batch_first=True)\n",
    "\n",
    "    def forward(self, x, key_padding_mask=None):\n",
    "        # x: [batch_size, seq_len, hidden_size]\n",
    "        # Expand query to [batch_size, 1, hidden_size]\n",
    "        query = self.query.expand(x.size(0), 1, -1)\n",
    "        # Apply attention, key_padding_mask is None for vision, used for text\n",
    "        attn_output, _ = self.attn(query, x, x, key_padding_mask=key_padding_mask)\n",
    "        # Output: [batch_size, 1, hidden_size] -> [batch_size, hidden_size]\n",
    "        return attn_output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siglip2VisionModel(PreTrainedModel):\n",
    "    config_class = Siglip2VisionConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Patch embedding to convert image to patches\n",
    "        self.patch_embedding = nn.Conv2d(\n",
    "            in_channels=config.num_channels,\n",
    "            out_channels=config.hidden_size,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size\n",
    "        )\n",
    "        # Number of patches: (image_size / patch_size)²\n",
    "        num_patches = (config.image_size // config.patch_size) ** 2\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n",
    "        # Transformer layers\n",
    "        self.transformer = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config.hidden_size,\n",
    "                nhead=config.num_attention_heads,\n",
    "                dim_feedforward=config.intermediate_size,\n",
    "                batch_first=True\n",
    "            ) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.map_head = AttentionPooling(config.hidden_size)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        # pixel_values: [batch_size, num_channels, image_size, image_size]\n",
    "        x = self.patch_embedding(pixel_values)  # [batch_size, hidden_size, H/p, W/p]\n",
    "        x = x.flatten(2).transpose(1, 2)  # [batch_size, num_patches, hidden_size]\n",
    "        x = x + self.positional_embedding\n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x)\n",
    "        # Pool using attention, no padding mask needed for vision\n",
    "        pooled_output = self.map_head(x)\n",
    "        return pooled_output  # [batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siglip2TextModel(PreTrainedModel):\n",
    "    config_class = Siglip2TextConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # Token embeddings\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_embeddings = nn.Parameter(\n",
    "            torch.zeros(1, config.max_length, config.hidden_size)\n",
    "        )\n",
    "        # Transformer layers\n",
    "        self.transformer = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=config.hidden_size,\n",
    "                nhead=config.num_attention_heads,\n",
    "                dim_feedforward=config.intermediate_size,\n",
    "                batch_first=True\n",
    "            ) for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.map_head = AttentionPooling(config.hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        x = self.embeddings(input_ids)  # [batch_size, seq_len, hidden_size]\n",
    "        x = x + self.positional_embeddings[:, :x.size(1), :]\n",
    "        # Handle padding mask\n",
    "        if attention_mask is not None:\n",
    "            src_key_padding_mask = (attention_mask == 0)  # True for padding tokens\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer:\n",
    "            x = layer(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        # Pool using attention, pass padding mask\n",
    "        pooled_output = self.map_head(x, key_padding_mask=src_key_padding_mask)\n",
    "        return pooled_output  # [batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siglip2Model(PreTrainedModel):\n",
    "    config_class = Siglip2Config\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = Siglip2VisionModel(config.vision_config)\n",
    "        self.text_model = Siglip2TextModel(config.text_config)\n",
    "        self.vision_projection = nn.Linear(\n",
    "            config.vision_config.hidden_size, config.projection_dim\n",
    "        )\n",
    "        self.text_projection = nn.Linear(\n",
    "            config.text_config.hidden_size, config.projection_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask=None):\n",
    "        # Get embeddings from vision and text models\n",
    "        vision_embeddings = self.vision_model(pixel_values)  # [batch_size, hidden_size]\n",
    "        text_embeddings = self.text_model(input_ids, attention_mask)  # [batch_size, hidden_size]\n",
    "        # Project to shared space\n",
    "        vision_proj = self.vision_projection(vision_embeddings)  # [batch_size, projection_dim]\n",
    "        text_proj = self.text_projection(text_embeddings)  # [batch_size, projection_dim]\n",
    "        return vision_proj, text_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siglip_loss(vision_proj, text_proj, temperature=1.0):\n",
    "    # Compute similarity logits\n",
    "    logits = torch.matmul(vision_proj, text_proj.t()) / temperature  # [batch_size, batch_size]\n",
    "    # Labels: diagonal = matching pairs\n",
    "    labels = torch.eye(logits.size(0), device=logits.device)\n",
    "    # Symmetric sigmoid loss\n",
    "    loss_i2t = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "    loss_t2i = F.binary_cross_entropy_with_logits(logits.t(), labels)\n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configurations\n",
    "vision_config = Siglip2VisionConfig()\n",
    "text_config = Siglip2TextConfig()\n",
    "config = Siglip2Config(vision_config=vision_config, text_config=text_config)\n",
    "\n",
    "# Initialize model\n",
    "model = Siglip2Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inputs (batch_size=8)\n",
    "pixel_values = torch.rand(8, 3, 224, 224)  # [batch_size, channels, height, width]\n",
    "input_ids = torch.randint(0, 32000, (8, 64))  # [batch_size, seq_len]\n",
    "attention_mask = torch.ones(8, 64)  # [batch_size, seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "vision_proj, text_proj = model(pixel_values, input_ids, attention_mask)\n",
    "\n",
    "# Compute loss\n",
    "loss = siglip_loss(vision_proj, text_proj)\n",
    "\n",
    "print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PretrainedConfig, PreTrainedModel, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CUDA kontrolü\n",
    "print(\"Checking CUDA availability...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "else:\n",
    "    print(\"CUDA not available, falling back to CPU\")\n",
    "    exit()\n",
    "\n",
    "# Dataset yükleme\n",
    "print(\"Loading Flickr30k dataset...\")\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "full_dataset = dataset[\"test\"]\n",
    "print(f\"Full dataset size: {len(full_dataset)} examples\")\n",
    "\n",
    "# Subset oluşturma\n",
    "print(\"Creating training and validation subsets...\")\n",
    "random.seed(42)\n",
    "all_indices = list(range(len(full_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "train_indices = all_indices[:100]\n",
    "val_indices = all_indices[100:150]\n",
    "train_subset = full_dataset.select(train_indices)\n",
    "val_subset = full_dataset.select(val_indices)\n",
    "print(f\"Training subset size: {len(train_subset)}\")\n",
    "print(f\"Validation subset size: {len(val_subset)}\")\n",
    "\n",
    "# Tokenizer\n",
    "print(\"Initializing BertTokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Tokenizer initialized!\")\n",
    "\n",
    "# Image transformations\n",
    "print(\"Defining image transformations...\")\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "print(\"Image transformations defined!\")\n",
    "\n",
    "# Custom Dataset (Düzeltildi)\n",
    "print(\"Defining custom Flickr30kMiniDataset class...\")\n",
    "class Flickr30kMiniDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        print(f\"Dataset initialized with {len(dataset)} examples\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        print(f\"Processing example at index {idx}\")\n",
    "        # Görüntü 'image' anahtarında\n",
    "        image = example[\"image\"]\n",
    "        print(f\"Image retrieved from dataset, type: {type(image)}\")\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "            print(\"Image converted to RGB\")\n",
    "        pixel_values = self.transform(image)\n",
    "        print(f\"Image transformed, pixel_values shape: {pixel_values.shape}\")\n",
    "        # İlk caption’ı al (liste içinden)\n",
    "        caption = example[\"caption\"][0]  # Düzeltme burada!\n",
    "        print(f\"Tokenizing caption: {caption}\")\n",
    "        tokenized = self.tokenizer(caption, padding=\"max_length\", max_length=64, truncation=True, return_tensors=\"pt\")\n",
    "        print(f\"Caption tokenized, input_ids shape: {tokenized['input_ids'].shape}\")\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Datasets oluşturma\n",
    "print(\"Creating training dataset...\")\n",
    "train_data = Flickr30kMiniDataset(train_subset, image_transform, tokenizer)\n",
    "print(\"Creating validation dataset...\")\n",
    "val_data = Flickr30kMiniDataset(val_subset, image_transform, tokenizer)\n",
    "\n",
    "# Data Loaders\n",
    "print(\"Creating training data loader...\")\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "print(f\"Training loader created with {len(train_loader)} batches\")\n",
    "print(\"Creating validation data loader...\")\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n",
    "print(f\"Validation loader created with {len(val_loader)} batches\")\n",
    "\n",
    "# [Paste your Siglip2VisionConfig, Siglip2TextConfig, Siglip2Config, AttentionPooling,\n",
    "# Siglip2VisionModel, Siglip2TextModel, Siglip2Model, and siglip_loss definitions here]\n",
    "\n",
    "# Model başlatma\n",
    "print(\"Initializing SIGLIP2 model...\")\n",
    "vision_config = Siglip2VisionConfig()\n",
    "text_config = Siglip2TextConfig()\n",
    "config = Siglip2Config(vision_config=vision_config, text_config=text_config)\n",
    "model = Siglip2Model(config)\n",
    "model.to(device)\n",
    "print(\"Model initialized and moved to CUDA!\")\n",
    "\n",
    "# Training setup\n",
    "print(\"Setting up training environment...\")\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "print(\"Optimizer initialized!\")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader))\n",
    "print(\"Scheduler initialized!\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "print(f\"Starting training for {num_epochs} epoch on {device}...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} started\")\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    print(\"Entering training loop...\")\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
    "        print(f\"Batch {batch_idx+1}/{len(train_loader)} loaded\")\n",
    "        pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        print(f\"Batch moved to {device} - pixel_values: {pixel_values.shape}, input_ids: {input_ids.shape}\")\n",
    "        vision_proj, text_proj = model(pixel_values, input_ids, attention_mask)\n",
    "        print(f\"Forward pass completed on {device}, vision_proj shape: {vision_proj.shape}\")\n",
    "        loss = siglip_loss(vision_proj, text_proj)\n",
    "        print(f\"Loss computed on {device}: {loss.item()}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        print(\"Backward pass completed\")\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        print(\"Optimizer step completed\")\n",
    "        total_train_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed, Average Training Loss: {avg_train_loss}\")\n",
    "\n",
    "    # Validation\n",
    "    print(f\"Starting validation for epoch {epoch+1}\")\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")):\n",
    "            print(f\"Validation batch {batch_idx+1}/{len(val_loader)} loaded\")\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            print(f\"Validation batch moved to {device} - pixel_values: {pixel_values.shape}\")\n",
    "            vision_proj, text_proj = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = siglip_loss(vision_proj, text_proj)\n",
    "            print(f\"Validation loss on {device}: {loss.item()}\")\n",
    "            total_val_loss += loss.item()\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    print(\"Starting model evaluation...\")\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(tqdm(data_loader, desc=\"Evaluation\")):\n",
    "            print(f\"Evaluation batch {batch_idx+1}/{len(data_loader)} loaded\")\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            vision_proj, text_proj = model(pixel_values, input_ids, attention_mask)\n",
    "            loss = siglip_loss(vision_proj, text_proj)\n",
    "            print(f\"Evaluation batch loss on {device}: {loss.item()}\")\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Evaluation completed, Average Loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Running final evaluation...\")\n",
    "evaluate_model(model, val_loader, device)\n",
    "print(\"Training and evaluation process finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import PretrainedConfig, PreTrainedModel, BertTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CUDA kontrolü\n",
    "print(\"Checking CUDA availability...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Dataset yükleme\n",
    "dataset = load_dataset(\"nlphuji/flickr30k\")\n",
    "full_dataset = dataset[\"test\"]\n",
    "random.seed(42)\n",
    "all_indices = list(range(len(full_dataset)))\n",
    "random.shuffle(all_indices)\n",
    "train_indices = all_indices[:100]\n",
    "val_indices = all_indices[100:150]\n",
    "train_subset = full_dataset.select(train_indices)\n",
    "val_subset = full_dataset.select(val_indices)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Image transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class Flickr30kMiniDataset(Dataset):\n",
    "    def __init__(self, dataset, transform, tokenizer):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        image = example[\"image\"]\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        pixel_values = self.transform(image)\n",
    "        caption = example[\"caption\"][0]  # İlk caption\n",
    "        tokenized = self.tokenizer(caption, padding=\"max_length\", max_length=64, truncation=True, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "# Data Loaders\n",
    "train_data = Flickr30kMiniDataset(train_subset, image_transform, tokenizer)\n",
    "val_data = Flickr30kMiniDataset(val_subset, image_transform, tokenizer)\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_data, batch_size=16, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Model Configs unchanged (Siglip2VisionConfig, Siglip2TextConfig, AttentionPooling as before)\n",
    "# [Paste your previous Siglip2VisionConfig, Siglip2TextConfig, AttentionPooling here]\n",
    "\n",
    "# Text Decoder (Image-to-Text için)\n",
    "class Siglip2Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_layers=6, num_heads=12, vocab_size=30522):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.decoder = nn.ModuleList([\n",
    "            nn.TransformerDecoderLayer(d_model=hidden_size, nhead=num_heads, dim_feedforward=hidden_size*4, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "        self.positional_embeddings = nn.Parameter(torch.zeros(1, 64, hidden_size))\n",
    "\n",
    "    def forward(self, vision_embedding, target_ids=None, attention_mask=None):\n",
    "        if target_ids is None:  # Inference\n",
    "            seq_len = 64\n",
    "            output_ids = torch.zeros((vision_embedding.size(0), seq_len), dtype=torch.long, device=vision_embedding.device)\n",
    "            for t in range(seq_len):\n",
    "                x = self.embedding(output_ids[:, :t+1]) + self.positional_embeddings[:, :t+1]\n",
    "                for layer in self.decoder:\n",
    "                    x = layer(x, vision_embedding, tgt_mask=nn.Transformer.generate_square_subsequent_mask(t+1).to(device))\n",
    "                logits = self.fc_out(x[:, -1, :])\n",
    "                output_ids[:, t] = logits.argmax(-1)\n",
    "            return output_ids\n",
    "        else:  # Training\n",
    "            x = self.embedding(target_ids) + self.positional_embeddings[:, :target_ids.size(1)]\n",
    "            for layer in self.decoder:\n",
    "                x = layer(x, vision_embedding, tgt_mask=nn.Transformer.generate_square_subsequent_mask(target_ids.size(1)).to(device))\n",
    "            return self.fc_out(x)\n",
    "\n",
    "# Simple Diffusion Model (Text-to-Image için temel)\n",
    "class SimpleDiffusion(nn.Module):\n",
    "    def __init__(self, hidden_size=768, img_size=224):\n",
    "        super().__init__()\n",
    "        self.noise_scheduler = torch.linspace(0, 1, 1000)  # Basit bir noise schedule\n",
    "        self.unet = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.text_proj = nn.Linear(hidden_size, 64*img_size*img_size)\n",
    "\n",
    "    def forward(self, text_embedding, steps=50):\n",
    "        noise = torch.randn(text_embedding.size(0), 3, 224, 224, device=device)\n",
    "        for t in range(steps):\n",
    "            t_tensor = torch.full((text_embedding.size(0),), self.noise_scheduler[t], device=device)\n",
    "            pred_noise = self.unet(noise + self.text_proj(text_embedding).view(-1, 64, 224, 224))\n",
    "            noise = noise - 0.1 * pred_noise  # Basit denoising\n",
    "        return noise.clamp(-1, 1)\n",
    "\n",
    "# Extended SIGLIP2 Model\n",
    "class Siglip2ModelExtended(PreTrainedModel):\n",
    "    config_class = Siglip2Config\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.vision_model = Siglip2VisionModel(config.vision_config)\n",
    "        self.text_model = Siglip2TextModel(config.text_config)\n",
    "        self.vision_projection = nn.Linear(config.vision_config.hidden_size, config.projection_dim)\n",
    "        self.text_projection = nn.Linear(config.text_config.hidden_size, config.projection_dim)\n",
    "        # Yeni eklenenler\n",
    "        self.decoder = Siglip2Decoder(config.text_config.hidden_size, vocab_size=tokenizer.vocab_size)\n",
    "        self.diffusion = SimpleDiffusion(config.projection_dim)\n",
    "\n",
    "    def forward(self, pixel_values, input_ids=None, attention_mask=None, mode=\"contrastive\"):\n",
    "        vision_embeddings = self.vision_model(pixel_values)\n",
    "        vision_proj = self.vision_projection(vision_embeddings)\n",
    "        \n",
    "        if mode == \"contrastive\":\n",
    "            text_embeddings = self.text_model(input_ids, attention_mask)\n",
    "            text_proj = self.text_projection(text_embeddings)\n",
    "            return vision_proj, text_proj\n",
    "        elif mode == \"image_to_text\":\n",
    "            return self.decoder(vision_proj, input_ids, attention_mask)\n",
    "        elif mode == \"text_to_image\":\n",
    "            text_embeddings = self.text_model(input_ids, attention_mask)\n",
    "            text_proj = self.text_projection(text_embeddings)\n",
    "            return self.diffusion(text_proj)\n",
    "\n",
    "# Training Setup\n",
    "config = Siglip2Config()\n",
    "model = Siglip2ModelExtended(config)\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=len(train_loader))\n",
    "\n",
    "# Loss Functions\n",
    "def contrastive_loss(vision_proj, text_proj):\n",
    "    return siglip_loss(vision_proj, text_proj)  # Previous siglip_loss\n",
    "\n",
    "def captioning_loss(logits, target_ids):\n",
    "    return F.cross_entropy(logits.view(-1, logits.size(-1)), target_ids.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "def diffusion_loss(pred_img, target_img):\n",
    "    return F.mse_loss(pred_img, target_img)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        # Contrastive Loss\n",
    "        vision_proj, text_proj = model(pixel_values, input_ids, attention_mask, mode=\"contrastive\")\n",
    "        loss_contrastive = contrastive_loss(vision_proj, text_proj)\n",
    "\n",
    "        # Captioning Loss\n",
    "        caption_logits = model(pixel_values, input_ids, attention_mask, mode=\"image_to_text\")\n",
    "        loss_caption = captioning_loss(caption_logits, input_ids)\n",
    "\n",
    "        # Diffusion Loss (Text-to-Image)\n",
    "        generated_img = model(input_ids=input_ids, attention_mask=attention_mask, mode=\"text_to_image\")\n",
    "        loss_diffusion = diffusion_loss(generated_img, pixel_values)\n",
    "\n",
    "        # Total Loss\n",
    "        loss = loss_contrastive + loss_caption + loss_diffusion\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Training Loss: {avg_loss}\")\n",
    "\n",
    "    # Validation (contrastive only for simplicity)\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(device, non_blocking=True)\n",
    "            input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "            vision_proj, text_proj = model(pixel_values, input_ids, attention_mask, mode=\"contrastive\")\n",
    "            loss = contrastive_loss(vision_proj, text_proj)\n",
    "            total_val_loss += loss.item()\n",
    "    print(f\"Validation Loss: {total_val_loss / len(val_loader)}\")\n",
    "\n",
    "# Inference Functions\n",
    "def image_to_text(model, image, tokenizer, max_length=64):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pixel_values = image_transform(image).unsqueeze(0).to(device)\n",
    "        caption_ids = model(pixel_values, mode=\"image_to_text\")\n",
    "        return tokenizer.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "def text_to_image(model, text, tokenizer):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(text, padding=\"max_length\", max_length=64, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = tokenized[\"input_ids\"].to(device)\n",
    "        attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "        generated_img = model(input_ids=input_ids, attention_mask=attention_mask, mode=\"text_to_image\")\n",
    "        return generated_img.squeeze(0).cpu()\n",
    "\n",
    "# Test Inference\n",
    "sample_image = val_subset[0][\"image\"]\n",
    "print(\"Generated Caption:\", image_to_text(model, sample_image, tokenizer))\n",
    "print(\"Generated Image from Text:\", text_to_image(model, \"A dog running in the park\", tokenizer).shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
