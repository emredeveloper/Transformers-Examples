{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42bd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "488f300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pixel_unshuffle(image, scale_factor=4):\n",
    "    \"\"\"\n",
    "    Pixel unshuffle iÅŸlemi uygular\n",
    "    scale_factor: Ã–lÃ§ek kÃ¼Ã§Ã¼ltme faktÃ¶rÃ¼ (genellikle 2, 4, 8 gibi)\n",
    "    \"\"\"\n",
    "    # Girdi boyutlarÄ±\n",
    "    B, C, H, W = image.shape\n",
    "    \n",
    "    # Yeni boyutlarÄ± hesapla\n",
    "    new_H = H // scale_factor\n",
    "    new_W = W // scale_factor\n",
    "    new_C = C * (scale_factor ** 2)\n",
    "    \n",
    "    # Pixel unshuffle uygula\n",
    "    unshuffled = F.pixel_unshuffle(image, scale_factor)\n",
    "    return unshuffled, (B, new_C, new_H, new_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005e90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLPConnector(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerMLPConnector, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f900c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientTokenMapper(nn.Module):\n",
    "    def __init__(self, unshuffle_factor=4, token_dim=768, hidden_dim=3072):\n",
    "        super(EfficientTokenMapper, self).__init__()\n",
    "        self.unshuffle_factor = unshuffle_factor\n",
    "        self.token_dim = token_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # MLP baÄŸlayÄ±cÄ±\n",
    "        self.mlp = TwoLayerMLPConnector(\n",
    "            input_dim=(3 * unshuffle_factor * unshuffle_factor),\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=token_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1. Pixel unshuffle uygula\n",
    "        x_unshuffled, shape_info = apply_pixel_unshuffle(x, self.unshuffle_factor)\n",
    "        \n",
    "        # 2. Token'lara yeniden ÅŸekil ver (Batch, Tokens, Channels)\n",
    "        B, C, H, W = x_unshuffled.shape\n",
    "        tokens = x_unshuffled.reshape(B, C, H * W).permute(0, 2, 1)\n",
    "        \n",
    "        # 3. MLP ile token mapping uygula\n",
    "        mapped_tokens = self.mlp(tokens)\n",
    "        \n",
    "        return mapped_tokens, (H, W), shape_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d11ffb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_image(image_path, img_size=(256, 384)):\n",
    "    \"\"\"GÃ¶rseli yÃ¼kle ve iÅŸle\"\"\"\n",
    "    # GÃ¶rseli yÃ¼kle\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    \n",
    "    # Transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # GÃ¶rseli iÅŸle\n",
    "    processed_image = transform(image).unsqueeze(0)  # Batch dimension ekle\n",
    "    return processed_image, image\n",
    "\n",
    "def visualize_results(original_img, processed_tensor, token_info):\n",
    "    \"\"\"SonuÃ§larÄ± gÃ¶rselleÅŸtir\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Orijinal gÃ¶rsel\n",
    "    axes[0].imshow(original_img)\n",
    "    axes[0].set_title(f'Orijinal GÃ¶rsel\\nBoyut: {original_img.size}')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Ä°ÅŸlenmiÅŸ tensor\n",
    "    axes[1].imshow(processed_tensor[0].permute(1, 2, 0))\n",
    "    axes[1].set_title(f'Ä°ÅŸlenmiÅŸ Tensor\\nShape: {processed_tensor.shape}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Token bilgisi\n",
    "    axes[2].text(0.1, 0.5, \n",
    "                f'Token SayÄ±sÄ±: {token_info[\"num_tokens\"]}\\n'\n",
    "                f'Token Boyutu: {token_info[\"token_dim\"]}\\n'\n",
    "                f'Unshuffle FaktÃ¶rÃ¼: {token_info[\"unshuffle_factor\"]}\\n'\n",
    "                f'Orijinal Piksel SayÄ±sÄ±: {token_info[\"original_pixels\"]}\\n'\n",
    "                f'Ä°ndirgeme OranÄ±: {token_info[\"reduction_ratio\"]:.1f}x',\n",
    "                fontsize=12, va='center')\n",
    "    axes[2].set_title('Token Bilgisi')\n",
    "    axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d9fdf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KullanÄ±lan cihaz: cuda\n",
      "Orijinal gÃ¶rsel boyutu: 256x384 = 98304 piksel\n",
      "Pixel unshuffle sonrasÄ±: (1, 48, 64, 96)\n",
      "Token sayÄ±sÄ±: 6144\n",
      "Her tokenÄ±n boyutu: 768\n",
      "Toplam Ã¶zellik sayÄ±sÄ±: 4718592\n",
      "Ä°ndirgeme oranÄ±: 16.0x\n",
      "\n",
      "GerÃ§ek bir gÃ¶rselle test etmek iÃ§in:\n",
      "1. YukarÄ±daki load_and_process_image fonksiyonunu kullanÄ±n\n",
      "2. dummy_image yerine gerÃ§ek gÃ¶rsel tensorÃ¼nÃ¼ verin\n",
      "3. visualize_results fonksiyonunu kullanÄ±n\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Cihaz ayarÄ±\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"KullanÄ±lan cihaz: {device}\")\n",
    "    \n",
    "    # GÃ¶rsel yÃ¼kle (kendi gÃ¶rsel yolunu verebilirsiniz)\n",
    "    # Ã–rnek olarak rastgele bir tensor kullanÄ±yoruz\n",
    "    original_height, original_width = 256, 384\n",
    "    dummy_image = torch.randn(1, 3, original_height, original_width).to(device)\n",
    "    \n",
    "    # Modeli oluÅŸtur\n",
    "    unshuffle_factor = 4\n",
    "    token_dim = 768\n",
    "    model = EfficientTokenMapper(\n",
    "        unshuffle_factor=unshuffle_factor,\n",
    "        token_dim=token_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Ä°ÅŸlemi uygula\n",
    "    with torch.no_grad():\n",
    "        mapped_tokens, spatial_dims, shape_info = model(dummy_image)\n",
    "    \n",
    "    # SonuÃ§larÄ± hesapla\n",
    "    H, W = spatial_dims\n",
    "    num_tokens = H * W\n",
    "    original_pixels = original_height * original_width\n",
    "    \n",
    "    token_info = {\n",
    "        'num_tokens': num_tokens,\n",
    "        'token_dim': token_dim,\n",
    "        'unshuffle_factor': unshuffle_factor,\n",
    "        'original_pixels': original_pixels,\n",
    "        'reduction_ratio': original_pixels / num_tokens\n",
    "    }\n",
    "    \n",
    "    # SonuÃ§larÄ± yazdÄ±r\n",
    "    print(f\"Orijinal gÃ¶rsel boyutu: {original_height}x{original_width} = {original_pixels} piksel\")\n",
    "    print(f\"Pixel unshuffle sonrasÄ±: {shape_info}\")\n",
    "    print(f\"Token sayÄ±sÄ±: {num_tokens}\")\n",
    "    print(f\"Her tokenÄ±n boyutu: {token_dim}\")\n",
    "    print(f\"Toplam Ã¶zellik sayÄ±sÄ±: {num_tokens * token_dim}\")\n",
    "    print(f\"Ä°ndirgeme oranÄ±: {token_info['reduction_ratio']:.1f}x\")\n",
    "    \n",
    "    # GÃ¶rselleÅŸtirme iÃ§in Ã¶rnek (gerÃ§ek gÃ¶rselle Ã§alÄ±ÅŸmak isterseniz)\n",
    "    print(\"\\nGerÃ§ek bir gÃ¶rselle test etmek iÃ§in:\")\n",
    "    print(\"1. YukarÄ±daki load_and_process_image fonksiyonunu kullanÄ±n\")\n",
    "    print(\"2. dummy_image yerine gerÃ§ek gÃ¶rsel tensorÃ¼nÃ¼ verin\")\n",
    "    print(\"3. visualize_results fonksiyonunu kullanÄ±n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9cad3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orijinal gÃ¶rsel: 256x384\n",
      "Token sayÄ±sÄ±: 6144\n",
      "Ä°ndirgeme oranÄ±: 16.0x\n",
      "6144 token oluÅŸturuldu!\n"
     ]
    }
   ],
   "source": [
    "def test_with_real_image(image_path):\n",
    "    \"\"\"GerÃ§ek bir gÃ¶rselle test edelim\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # GÃ¶rseli yÃ¼kle ve iÅŸle\n",
    "    processed_tensor, original_img = load_and_process_image(image_path)\n",
    "    processed_tensor = processed_tensor.to(device)\n",
    "    \n",
    "    # Modeli oluÅŸtur\n",
    "    unshuffle_factor = 4\n",
    "    token_dim = 768\n",
    "    model = EfficientTokenMapper(\n",
    "        unshuffle_factor=unshuffle_factor,\n",
    "        token_dim=token_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    # Ä°ÅŸlemi uygula\n",
    "    with torch.no_grad():\n",
    "        mapped_tokens, spatial_dims, _ = model(processed_tensor)\n",
    "    \n",
    "    H, W = spatial_dims\n",
    "    num_tokens = H * W\n",
    "    original_pixels = processed_tensor.shape[2] * processed_tensor.shape[3]\n",
    "    \n",
    "    print(f\"Orijinal gÃ¶rsel: {processed_tensor.shape[2]}x{processed_tensor.shape[3]}\")\n",
    "    print(f\"Token sayÄ±sÄ±: {num_tokens}\")\n",
    "    print(f\"Ä°ndirgeme oranÄ±: {original_pixels/num_tokens:.1f}x\")\n",
    "    \n",
    "    return mapped_tokens, num_tokens\n",
    "\n",
    "# Ã–rnek kullanÄ±m:\n",
    "tokens, count = test_with_real_image('YouTube-QA-Agent-08-22-2025_01_46_PM.png')\n",
    "print(f\"{count} token oluÅŸturuldu!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8db32",
   "metadata": {},
   "source": [
    "# ðŸš€ GeliÅŸtirebileceklerimiz\n",
    "\n",
    "## 1. Adaptif Token Mapping\n",
    "- **Dinamik Unshuffle Factor**: GÃ¶rÃ¼ntÃ¼ karmaÅŸÄ±klÄ±ÄŸÄ±na gÃ¶re otomatik ayarlama\n",
    "- **Attention-based Token Selection**: Ã–nemli bÃ¶lgelere daha fazla token\n",
    "- **Multi-scale Processing**: FarklÄ± resolution'larda token'lar\n",
    "\n",
    "## 2. Advanced Architecture Improvements\n",
    "- **Learnable Position Embedding**: Spatial bilgiyi koruma\n",
    "- **Cross-attention Mechanisms**: Multi-modal alignment\n",
    "- **Efficient Attention Patterns**: Linear attention, sparse attention\n",
    "\n",
    "## 3. Optimization Techniques\n",
    "- **Knowledge Distillation**: BÃ¼yÃ¼k modelden kÃ¼Ã§Ã¼k modele bilgi transferi\n",
    "- **Quantization**: Model boyutunu kÃ¼Ã§Ã¼ltme\n",
    "- **Pruning**: Gereksiz parametreleri kaldÄ±rma\n",
    "\n",
    "## 4. Multi-Modal Extensions\n",
    "- **Text-Image Fusion**: CLIP tarzÄ± joint embeddings\n",
    "- **Video Processing**: Temporal dimension ekleme\n",
    "- **Audio-Visual**: Ã‡oklu modalite desteÄŸi\n",
    "\n",
    "## 5. Real-world Applications\n",
    "- **Fine-tuning Pipelines**: Specific task'lar iÃ§in adaptasyon\n",
    "- **Deployment Optimization**: Edge device'lar iÃ§in optimizasyon\n",
    "- **Benchmarking**: Standart dataset'lerde performance Ã¶lÃ§Ã¼mÃ¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa685ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "was expecting embedding dimension of 48, but got 12",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 79\u001b[39m\n\u001b[32m     76\u001b[39m         tokens2, dims2, attn2, factor2 = adaptive_model(complex_image)\n\u001b[32m     77\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mKarmaÅŸÄ±k gÃ¶rÃ¼ntÃ¼ - Unshuffle factor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfactor2\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Token sayÄ±sÄ±: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims2[\u001b[32m0\u001b[39m]*dims2[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m \u001b[43mtest_adaptive_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtest_adaptive_mapping\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     68\u001b[39m adaptive_model = AdaptiveTokenMapper().to(device)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     71\u001b[39m     \u001b[38;5;66;03m# Simple image test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     tokens1, dims1, attn1, factor1 = \u001b[43madaptive_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimple_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBasit gÃ¶rÃ¼ntÃ¼ - Unshuffle factor: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfactor1\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Token sayÄ±sÄ±: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdims1[\u001b[32m0\u001b[39m]*dims1[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m     \u001b[38;5;66;03m# Complex image test  \u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emreq\\Desktop\\Projeler\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emreq\\Desktop\\Projeler\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mAdaptiveTokenMapper.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     49\u001b[39m tokens = x_unshuffled.reshape(B, C, H * W).permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# Attention mechanism ile Ã¶nemli token'larÄ± belirle\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m attended_tokens, attention_weights = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Adaptive MLP seÃ§imi\u001b[39;00m\n\u001b[32m     55\u001b[39m mlp_key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munshuffle_factor\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emreq\\Desktop\\Projeler\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emreq\\Desktop\\Projeler\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emreq\\Desktop\\Projeler\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1373\u001b[39m, in \u001b[36mMultiheadAttention.forward\u001b[39m\u001b[34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   1347\u001b[39m     attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[32m   1348\u001b[39m         query,\n\u001b[32m   1349\u001b[39m         key,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1370\u001b[39m         is_causal=is_causal,\n\u001b[32m   1371\u001b[39m     )\n\u001b[32m   1372\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1373\u001b[39m     attn_output, attn_output_weights = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mout_proj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[32m   1395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m), attn_output_weights\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\emreq\\Desktop\\Projeler\\Transformers\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:6203\u001b[39m, in \u001b[36mmulti_head_attention_forward\u001b[39m\u001b[34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[39m\n\u001b[32m   6196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   6197\u001b[39m         \u001b[38;5;66;03m# We have the attn_mask, and use that to merge kpm into it.\u001b[39;00m\n\u001b[32m   6198\u001b[39m         \u001b[38;5;66;03m# Turn off use of is_causal hint, as the merged mask is no\u001b[39;00m\n\u001b[32m   6199\u001b[39m         \u001b[38;5;66;03m# longer causal.\u001b[39;00m\n\u001b[32m   6200\u001b[39m         is_causal = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   6202\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m6203\u001b[39m     embed_dim == embed_dim_to_check\n\u001b[32m   6204\u001b[39m ), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mwas expecting embedding dimension of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim_to_check\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   6205\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_dim, torch.Tensor):\n\u001b[32m   6206\u001b[39m     \u001b[38;5;66;03m# embed_dim can be a tensor when JIT tracing\u001b[39;00m\n\u001b[32m   6207\u001b[39m     head_dim = embed_dim.div(num_heads, rounding_mode=\u001b[33m\"\u001b[39m\u001b[33mtrunc\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAssertionError\u001b[39m: was expecting embedding dimension of 48, but got 12"
     ]
    }
   ],
   "source": [
    "# 1. Adaptif Token Mapping Implementasyonu\n",
    "class AdaptiveTokenMapper(nn.Module):\n",
    "    def __init__(self, base_unshuffle=4, token_dim=768, num_heads=8):\n",
    "        super(AdaptiveTokenMapper, self).__init__()\n",
    "        self.base_unshuffle = base_unshuffle\n",
    "        self.token_dim = token_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Complexity analyzer\n",
    "        self.complexity_conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "        self.complexity_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Multi-head attention for token importance\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=3 * base_unshuffle * base_unshuffle,\n",
    "            num_heads=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Adaptive MLP layers\n",
    "        self.adaptive_mlp = nn.ModuleDict({\n",
    "            '2x': TwoLayerMLPConnector(3*4, 1024, token_dim),\n",
    "            '4x': TwoLayerMLPConnector(3*16, 2048, token_dim),\n",
    "            '8x': TwoLayerMLPConnector(3*64, 4096, token_dim)\n",
    "        })\n",
    "        \n",
    "    def analyze_complexity(self, x):\n",
    "        \"\"\"GÃ¶rÃ¼ntÃ¼ karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± analiz et\"\"\"\n",
    "        complexity = self.complexity_conv(x)\n",
    "        complexity = self.complexity_pool(complexity).squeeze()\n",
    "        \n",
    "        # Complexity score'a gÃ¶re unshuffle factor belirle\n",
    "        if complexity < 0.3:\n",
    "            return 2  # Basit gÃ¶rÃ¼ntÃ¼ler iÃ§in\n",
    "        elif complexity < 0.7:\n",
    "            return 4  # Orta karmaÅŸÄ±klÄ±k\n",
    "        else:\n",
    "            return 8  # KarmaÅŸÄ±k gÃ¶rÃ¼ntÃ¼ler iÃ§in\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Adaptive unshuffle factor\n",
    "        unshuffle_factor = self.analyze_complexity(x)\n",
    "        \n",
    "        # Pixel unshuffle uygula\n",
    "        x_unshuffled, shape_info = apply_pixel_unshuffle(x, unshuffle_factor)\n",
    "        \n",
    "        # Token'lara dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "        B, C, H, W = x_unshuffled.shape\n",
    "        tokens = x_unshuffled.reshape(B, C, H * W).permute(0, 2, 1)\n",
    "        \n",
    "        # Attention mechanism ile Ã¶nemli token'larÄ± belirle\n",
    "        attended_tokens, attention_weights = self.attention(tokens, tokens, tokens)\n",
    "        \n",
    "        # Adaptive MLP seÃ§imi\n",
    "        mlp_key = f\"{unshuffle_factor}x\"\n",
    "        mapped_tokens = self.adaptive_mlp[mlp_key](attended_tokens)\n",
    "        \n",
    "        return mapped_tokens, (H, W), attention_weights, unshuffle_factor\n",
    "\n",
    "# Test adaptive mapper\n",
    "def test_adaptive_mapping():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Test with different complexity images\n",
    "    simple_image = torch.ones(1, 3, 256, 256).to(device) * 0.5  # Uniform image\n",
    "    complex_image = torch.randn(1, 3, 256, 256).to(device)      # Random noise\n",
    "    \n",
    "    adaptive_model = AdaptiveTokenMapper().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Simple image test\n",
    "        tokens1, dims1, attn1, factor1 = adaptive_model(simple_image)\n",
    "        print(f\"Basit gÃ¶rÃ¼ntÃ¼ - Unshuffle factor: {factor1}, Token sayÄ±sÄ±: {dims1[0]*dims1[1]}\")\n",
    "        \n",
    "        # Complex image test  \n",
    "        tokens2, dims2, attn2, factor2 = adaptive_model(complex_image)\n",
    "        print(f\"KarmaÅŸÄ±k gÃ¶rÃ¼ntÃ¼ - Unshuffle factor: {factor2}, Token sayÄ±sÄ±: {dims2[0]*dims2[1]}\")\n",
    "\n",
    "test_adaptive_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec64e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Multi-Scale Token Processing\n",
    "class MultiScaleTokenMapper(nn.Module):\n",
    "    def __init__(self, scales=[2, 4, 8], token_dim=768):\n",
    "        super(MultiScaleTokenMapper, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.token_dim = token_dim\n",
    "        \n",
    "        # Her scale iÃ§in ayrÄ± mapper\n",
    "        self.scale_mappers = nn.ModuleDict()\n",
    "        for scale in scales:\n",
    "            input_dim = 3 * scale * scale\n",
    "            self.scale_mappers[f\"scale_{scale}\"] = TwoLayerMLPConnector(\n",
    "                input_dim=input_dim,\n",
    "                hidden_dim=input_dim * 4,\n",
    "                output_dim=token_dim\n",
    "            )\n",
    "        \n",
    "        # Scale fusion mechanism\n",
    "        self.scale_fusion = nn.MultiheadAttention(\n",
    "            embed_dim=token_dim,\n",
    "            num_heads=8,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final projection\n",
    "        self.final_proj = nn.Linear(token_dim, token_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        scale_tokens = []\n",
    "        scale_dims = []\n",
    "        \n",
    "        # Her scale iÃ§in token'lar oluÅŸtur\n",
    "        for scale in self.scales:\n",
    "            # Pixel unshuffle uygula\n",
    "            x_unshuffled, shape_info = apply_pixel_unshuffle(x, scale)\n",
    "            _, C, H, W = x_unshuffled.shape\n",
    "            \n",
    "            # Token'lara dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "            tokens = x_unshuffled.reshape(B, C, H * W).permute(0, 2, 1)\n",
    "            \n",
    "            # Scale-specific mapping\n",
    "            mapped = self.scale_mappers[f\"scale_{scale}\"](tokens)\n",
    "            scale_tokens.append(mapped)\n",
    "            scale_dims.append((H, W))\n",
    "        \n",
    "        # En kÃ¼Ã§Ã¼k scale'i referans al (en fazla token sayÄ±sÄ±)\n",
    "        max_tokens = max([t.shape[1] for t in scale_tokens])\n",
    "        \n",
    "        # TÃ¼m scale'leri aynÄ± token sayÄ±sÄ±na getir (interpolation)\n",
    "        aligned_tokens = []\n",
    "        for i, tokens in enumerate(scale_tokens):\n",
    "            if tokens.shape[1] != max_tokens:\n",
    "                # Adaptive pooling ile token sayÄ±sÄ±nÄ± eÅŸitle\n",
    "                tokens_reshaped = tokens.permute(0, 2, 1)  # (B, token_dim, num_tokens)\n",
    "                tokens_pooled = F.adaptive_avg_pool1d(tokens_reshaped, max_tokens)\n",
    "                tokens = tokens_pooled.permute(0, 2, 1)  # (B, num_tokens, token_dim)\n",
    "            aligned_tokens.append(tokens)\n",
    "        \n",
    "        # Scale'leri birleÅŸtir\n",
    "        stacked_tokens = torch.stack(aligned_tokens, dim=2)  # (B, num_tokens, num_scales, token_dim)\n",
    "        B, num_tokens, num_scales, token_dim = stacked_tokens.shape\n",
    "        \n",
    "        # Attention ile scale fusion\n",
    "        fused_tokens = stacked_tokens.view(B * num_tokens, num_scales, token_dim)\n",
    "        fused_output, _ = self.scale_fusion(fused_tokens, fused_tokens, fused_tokens)\n",
    "        \n",
    "        # Scale dimension'Ä± birleÅŸtir (ortalama al)\n",
    "        final_tokens = fused_output.mean(dim=1)  # (B * num_tokens, token_dim)\n",
    "        final_tokens = final_tokens.view(B, num_tokens, token_dim)\n",
    "        \n",
    "        # Final projection\n",
    "        output_tokens = self.final_proj(final_tokens)\n",
    "        \n",
    "        return output_tokens, scale_dims[0], stacked_tokens.shape\n",
    "\n",
    "# Test multi-scale processing\n",
    "def test_multiscale_processing():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Test image\n",
    "    test_image = torch.randn(1, 3, 256, 256).to(device)\n",
    "    \n",
    "    # Multi-scale model\n",
    "    multiscale_model = MultiScaleTokenMapper().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        tokens, dims, shape_info = multiscale_model(test_image)\n",
    "        \n",
    "    print(f\"Multi-scale processing:\")\n",
    "    print(f\"Input shape: {test_image.shape}\")\n",
    "    print(f\"Output tokens: {tokens.shape}\")\n",
    "    print(f\"Spatial dimensions: {dims}\")\n",
    "    print(f\"Multi-scale shape: {shape_info}\")\n",
    "\n",
    "test_multiscale_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bb1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Vision-Language Fusion (Multimodal)\n",
    "class VisionLanguageFusion(nn.Module):\n",
    "    def __init__(self, vision_dim=768, text_dim=768, fusion_dim=1024):\n",
    "        super(VisionLanguageFusion, self).__init__()\n",
    "        self.vision_dim = vision_dim\n",
    "        self.text_dim = text_dim\n",
    "        self.fusion_dim = fusion_dim\n",
    "        \n",
    "        # Vision token mapper (mevcut pixel unshuffle kullanarak)\n",
    "        self.vision_mapper = EfficientTokenMapper()\n",
    "        \n",
    "        # Text embedding (basit bir Ã¶rnek)\n",
    "        self.text_embedding = nn.Embedding(vocab_size=50000, embedding_dim=text_dim)\n",
    "        self.text_pos_embedding = nn.Parameter(torch.randn(1, 512, text_dim))\n",
    "        \n",
    "        # Cross-modal attention\n",
    "        self.vision_to_text_attn = nn.MultiheadAttention(\n",
    "            embed_dim=fusion_dim, num_heads=12, batch_first=True\n",
    "        )\n",
    "        self.text_to_vision_attn = nn.MultiheadAttention(\n",
    "            embed_dim=fusion_dim, num_heads=12, batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Projection layers\n",
    "        self.vision_proj = nn.Linear(vision_dim, fusion_dim)\n",
    "        self.text_proj = nn.Linear(text_dim, fusion_dim)\n",
    "        \n",
    "        # Fusion transformer\n",
    "        self.fusion_transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=fusion_dim,\n",
    "                nhead=16,\n",
    "                dim_feedforward=fusion_dim * 4,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=6\n",
    "        )\n",
    "        \n",
    "        # Output heads\n",
    "        self.classification_head = nn.Linear(fusion_dim, 1000)  # ImageNet classes\n",
    "        self.caption_head = nn.Linear(fusion_dim, 50000)        # Vocabulary\n",
    "        \n",
    "    def forward(self, image, text_tokens=None, mode='classification'):\n",
    "        B = image.shape[0]\n",
    "        \n",
    "        # Vision processing\n",
    "        vision_tokens, spatial_dims, _ = self.vision_mapper(image)\n",
    "        vision_tokens = self.vision_proj(vision_tokens)  # Project to fusion dim\n",
    "        \n",
    "        if text_tokens is not None and mode == 'multimodal':\n",
    "            # Text processing\n",
    "            text_embeds = self.text_embedding(text_tokens)\n",
    "            seq_len = text_embeds.shape[1]\n",
    "            text_embeds = text_embeds + self.text_pos_embedding[:, :seq_len, :]\n",
    "            text_embeds = self.text_proj(text_embeds)\n",
    "            \n",
    "            # Cross-modal attention\n",
    "            # Vision attending to text\n",
    "            v2t_output, _ = self.vision_to_text_attn(\n",
    "                vision_tokens, text_embeds, text_embeds\n",
    "            )\n",
    "            \n",
    "            # Text attending to vision\n",
    "            t2v_output, _ = self.text_to_vision_attn(\n",
    "                text_embeds, vision_tokens, vision_tokens\n",
    "            )\n",
    "            \n",
    "            # Concatenate for joint processing\n",
    "            joint_tokens = torch.cat([v2t_output, t2v_output], dim=1)\n",
    "        else:\n",
    "            # Vision-only mode\n",
    "            joint_tokens = vision_tokens\n",
    "        \n",
    "        # Fusion transformer\n",
    "        fused_features = self.fusion_transformer(joint_tokens)\n",
    "        \n",
    "        # Global pooling\n",
    "        pooled_features = fused_features.mean(dim=1)  # (B, fusion_dim)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        if mode == 'classification':\n",
    "            return self.classification_head(pooled_features)\n",
    "        elif mode == 'captioning':\n",
    "            return self.caption_head(fused_features)  # Return all tokens for sequence generation\n",
    "        else:\n",
    "            return pooled_features  # Return raw features\n",
    "\n",
    "# Test vision-language fusion\n",
    "def test_vision_language_fusion():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Test data\n",
    "    test_image = torch.randn(2, 3, 256, 256).to(device)\n",
    "    test_text = torch.randint(0, 1000, (2, 20)).to(device)  # 20 tokens per sample\n",
    "    \n",
    "    # Model\n",
    "    vl_model = VisionLanguageFusion().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Vision-only classification\n",
    "        cls_output = vl_model(test_image, mode='classification')\n",
    "        print(f\"Classification output shape: {cls_output.shape}\")\n",
    "        \n",
    "        # Multimodal processing\n",
    "        multimodal_features = vl_model(test_image, test_text, mode='multimodal')\n",
    "        print(f\"Multimodal features shape: {multimodal_features.shape}\")\n",
    "        \n",
    "        # Caption generation setup\n",
    "        caption_logits = vl_model(test_image, mode='captioning')\n",
    "        print(f\"Caption logits shape: {caption_logits.shape}\")\n",
    "\n",
    "test_vision_language_fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98991fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Performance Optimization Techniques\n",
    "import torch.quantization as quantization\n",
    "from torch.ao.quantization import get_default_qconfig\n",
    "\n",
    "class OptimizedEfficientTokenMapper(nn.Module):\n",
    "    \"\"\"Optimized version with quantization and efficient attention\"\"\"\n",
    "    \n",
    "    def __init__(self, unshuffle_factor=4, token_dim=768, use_flash_attention=True):\n",
    "        super(OptimizedEfficientTokenMapper, self).__init__()\n",
    "        self.unshuffle_factor = unshuffle_factor\n",
    "        self.token_dim = token_dim\n",
    "        self.use_flash_attention = use_flash_attention\n",
    "        \n",
    "        # Efficient MLP with grouped convolutions\n",
    "        self.efficient_mlp = nn.Sequential(\n",
    "            nn.Conv1d(3 * unshuffle_factor * unshuffle_factor, token_dim // 2, \n",
    "                     kernel_size=1, groups=4),  # Grouped conv for efficiency\n",
    "            nn.BatchNorm1d(token_dim // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(token_dim // 2, token_dim, kernel_size=1),\n",
    "            nn.BatchNorm1d(token_dim)\n",
    "        )\n",
    "        \n",
    "        # Linear attention for efficiency (O(n) instead of O(nÂ²))\n",
    "        self.linear_attention = LinearAttention(token_dim, num_heads=8)\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 10000, token_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pixel unshuffle\n",
    "        x_unshuffled, shape_info = apply_pixel_unshuffle(x, self.unshuffle_factor)\n",
    "        \n",
    "        # Reshape for conv1d processing\n",
    "        B, C, H, W = x_unshuffled.shape\n",
    "        tokens = x_unshuffled.reshape(B, C, H * W)  # (B, C, num_tokens)\n",
    "        \n",
    "        # Efficient MLP processing\n",
    "        mapped_tokens = self.efficient_mlp(tokens)  # (B, token_dim, num_tokens)\n",
    "        mapped_tokens = mapped_tokens.permute(0, 2, 1)  # (B, num_tokens, token_dim)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        num_tokens = mapped_tokens.shape[1]\n",
    "        mapped_tokens = mapped_tokens + self.pos_embedding[:, :num_tokens, :]\n",
    "        \n",
    "        # Linear attention\n",
    "        if self.use_flash_attention:\n",
    "            attended_tokens = self.linear_attention(mapped_tokens)\n",
    "        else:\n",
    "            attended_tokens = mapped_tokens\n",
    "            \n",
    "        return attended_tokens, (H, W), shape_info\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"Linear attention mechanism for O(n) complexity\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        \n",
    "        # Linear attention: softmax on key dimension\n",
    "        k = k.softmax(dim=-2)\n",
    "        context = torch.einsum('bhnd,bhne->bhde', k, v)\n",
    "        out = torch.einsum('bhnd,bhde->bhne', q, context)\n",
    "        \n",
    "        out = out.transpose(1, 2).reshape(B, N, C)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "# Quantization utility\n",
    "def quantize_model(model, example_input):\n",
    "    \"\"\"Model quantization for deployment\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Post-training quantization\n",
    "    model_int8 = torch.quantization.quantize_dynamic(\n",
    "        model, \n",
    "        {nn.Linear, nn.Conv1d, nn.Conv2d}, \n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "    \n",
    "    return model_int8\n",
    "\n",
    "# Benchmarking utility\n",
    "def benchmark_models():\n",
    "    \"\"\"Compare performance of different implementations\"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Test data\n",
    "    test_input = torch.randn(4, 3, 256, 256).to(device)\n",
    "    \n",
    "    models = {\n",
    "        'Original': EfficientTokenMapper().to(device),\n",
    "        'Optimized': OptimizedEfficientTokenMapper().to(device),\n",
    "        'Adaptive': AdaptiveTokenMapper().to(device),\n",
    "        'MultiScale': MultiScaleTokenMapper().to(device)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        model.eval()\n",
    "        \n",
    "        # Warmup\n",
    "        with torch.no_grad():\n",
    "            for _ in range(5):\n",
    "                if name == 'Adaptive':\n",
    "                    _ = model(test_input)\n",
    "                else:\n",
    "                    _ = model(test_input)\n",
    "        \n",
    "        # Timing\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = torch.cuda.Event(enable_timing=True) if device.type == 'cuda' else None\n",
    "        end_time = torch.cuda.Event(enable_timing=True) if device.type == 'cuda' else None\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            start_time.record()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                output = model(test_input)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            end_time.record()\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time = start_time.elapsed_time(end_time) / 10  # Average\n",
    "        else:\n",
    "            elapsed_time = 0  # Placeholder for CPU timing\n",
    "        \n",
    "        # Memory usage\n",
    "        if device.type == 'cuda':\n",
    "            memory_used = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "        else:\n",
    "            memory_used = 0\n",
    "        \n",
    "        # Model parameters\n",
    "        num_params = sum(p.numel() for p in model.parameters())\n",
    "        \n",
    "        results[name] = {\n",
    "            'inference_time_ms': elapsed_time,\n",
    "            'memory_mb': memory_used,\n",
    "            'parameters': num_params,\n",
    "            'output_shape': output[0].shape if isinstance(output, tuple) else output.shape\n",
    "        }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Model':<12} {'Time (ms)':<12} {'Memory (MB)':<12} {'Parameters':<12} {'Output Shape'}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for name, stats in results.items():\n",
    "        print(f\"{name:<12} {stats['inference_time_ms']:<12.2f} {stats['memory_mb']:<12.1f} \"\n",
    "              f\"{stats['parameters']:<12} {str(stats['output_shape'])}\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57960b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Real-World Application Demo\n",
      "==================================================\n",
      "1. Basic Deployment Test:\n",
      "âŒ Demo failed: name 'OptimizedEfficientTokenMapper' is not defined\n",
      "ðŸ’¡ Make sure you have the test image file available\n"
     ]
    }
   ],
   "source": [
    "# 5. Real-World Deployment Examples\n",
    "class DeploymentPipeline:\n",
    "    \"\"\"Production-ready deployment pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='optimized', quantized=True):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model_type = model_type\n",
    "        self.quantized = quantized\n",
    "        \n",
    "        # Load appropriate model\n",
    "        if model_type == 'optimized':\n",
    "            self.model = OptimizedEfficientTokenMapper().to(self.device)\n",
    "        elif model_type == 'adaptive':\n",
    "            self.model = AdaptiveTokenMapper().to(self.device)\n",
    "        else:\n",
    "            self.model = EfficientTokenMapper().to(self.device)\n",
    "        \n",
    "        # Apply quantization if requested\n",
    "        if quantized and self.device.type == 'cpu':\n",
    "            example_input = torch.randn(1, 3, 256, 256)\n",
    "            self.model = quantize_model(self.model, example_input)\n",
    "        \n",
    "        # Preprocessing pipeline\n",
    "        self.preprocess = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Post-processing utilities\n",
    "        self.setup_postprocessing()\n",
    "    \n",
    "    def setup_postprocessing(self):\n",
    "        \"\"\"Setup post-processing for different tasks\"\"\"\n",
    "        # ImageNet class names (sample)\n",
    "        self.imagenet_classes = [f\"class_{i}\" for i in range(1000)]\n",
    "        \n",
    "    def process_single_image(self, image_path):\n",
    "        \"\"\"Process a single image\"\"\"\n",
    "        try:\n",
    "            # Load and preprocess\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Inference\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                if self.model_type == 'adaptive':\n",
    "                    tokens, spatial_dims, attention_weights, unshuffle_factor = self.model(input_tensor)\n",
    "                    metadata = {\n",
    "                        'unshuffle_factor': unshuffle_factor,\n",
    "                        'attention_weights': attention_weights.cpu().numpy() if attention_weights is not None else None\n",
    "                    }\n",
    "                else:\n",
    "                    tokens, spatial_dims, _ = self.model(input_tensor)\n",
    "                    metadata = {}\n",
    "            \n",
    "            return {\n",
    "                'tokens': tokens.cpu().numpy(),\n",
    "                'spatial_dims': spatial_dims,\n",
    "                'metadata': metadata,\n",
    "                'success': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'error': str(e),\n",
    "                'success': False\n",
    "            }\n",
    "    \n",
    "    def batch_process(self, image_paths, batch_size=8):\n",
    "        \"\"\"Process multiple images in batches\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_tensors = []\n",
    "            \n",
    "            # Load batch\n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    image = Image.open(path).convert('RGB')\n",
    "                    tensor = self.preprocess(image)\n",
    "                    batch_tensors.append(tensor)\n",
    "                except:\n",
    "                    batch_tensors.append(torch.zeros(3, 256, 256))  # Fallback\n",
    "            \n",
    "            # Stack and process\n",
    "            batch_input = torch.stack(batch_tensors).to(self.device)\n",
    "            \n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                if self.model_type == 'adaptive':\n",
    "                    tokens, spatial_dims, _, _ = self.model(batch_input)\n",
    "                else:\n",
    "                    tokens, spatial_dims, _ = self.model(batch_input)\n",
    "            \n",
    "            # Split results\n",
    "            for j, path in enumerate(batch_paths):\n",
    "                results.append({\n",
    "                    'path': path,\n",
    "                    'tokens': tokens[j].cpu().numpy(),\n",
    "                    'spatial_dims': spatial_dims\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Application examples\n",
    "class VisionApplications:\n",
    "    \"\"\"Real-world application examples\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def image_similarity_search(query_image_path, database_paths, top_k=5):\n",
    "        \"\"\"Find similar images using token embeddings\"\"\"\n",
    "        pipeline = DeploymentPipeline(model_type='optimized')\n",
    "        \n",
    "        # Process query image\n",
    "        query_result = pipeline.process_single_image(query_image_path)\n",
    "        if not query_result['success']:\n",
    "            return None\n",
    "        \n",
    "        query_tokens = query_result['tokens']\n",
    "        query_features = query_tokens.mean(axis=1)  # Global pooling\n",
    "        \n",
    "        # Process database\n",
    "        database_results = pipeline.batch_process(database_paths)\n",
    "        similarities = []\n",
    "        \n",
    "        for result in database_results:\n",
    "            db_features = result['tokens'].mean(axis=1)\n",
    "            \n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_features, db_features) / (\n",
    "                np.linalg.norm(query_features) * np.linalg.norm(db_features)\n",
    "            )\n",
    "            similarities.append((result['path'], similarity))\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_k]\n",
    "    \n",
    "    @staticmethod\n",
    "    def content_based_retrieval(image_path, content_type='object'):\n",
    "        \"\"\"Content-based image analysis\"\"\"\n",
    "        pipeline = DeploymentPipeline(model_type='adaptive')\n",
    "        \n",
    "        result = pipeline.process_single_image(image_path)\n",
    "        if not result['success']:\n",
    "            return None\n",
    "        \n",
    "        tokens = result['tokens']\n",
    "        metadata = result['metadata']\n",
    "        \n",
    "        # Analyze token distribution\n",
    "        token_variance = np.var(tokens, axis=1).mean()\n",
    "        spatial_complexity = metadata.get('unshuffle_factor', 4)\n",
    "        \n",
    "        analysis = {\n",
    "            'complexity_score': token_variance,\n",
    "            'spatial_detail': spatial_complexity,\n",
    "            'content_type': 'complex' if token_variance > 0.5 else 'simple',\n",
    "            'recommended_processing': 'high_resolution' if spatial_complexity > 4 else 'standard'\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "# Example usage and testing\n",
    "def demo_real_world_applications():\n",
    "    \"\"\"Demonstrate real-world applications\"\"\"\n",
    "    print(\"ðŸš€ Real-World Application Demo\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create dummy test images (in real scenario, use actual image paths)\n",
    "    test_image_path = 'YouTube-QA-Agent-08-22-2025_01_46_PM.png'\n",
    "    \n",
    "    try:\n",
    "        # 1. Basic deployment\n",
    "        print(\"1. Basic Deployment Test:\")\n",
    "        pipeline = DeploymentPipeline(model_type='optimized', quantized=False)\n",
    "        result = pipeline.process_single_image(test_image_path)\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"   âœ… Token shape: {result['tokens'].shape}\")\n",
    "            print(f\"   âœ… Spatial dims: {result['spatial_dims']}\")\n",
    "        else:\n",
    "            print(f\"   âŒ Error: {result['error']}\")\n",
    "        \n",
    "        # 2. Content analysis\n",
    "        print(\"\\\\n2. Content Analysis:\")\n",
    "        analysis = VisionApplications.content_based_retrieval(test_image_path)\n",
    "        if analysis:\n",
    "            print(f\"   ðŸ“Š Complexity score: {analysis['complexity_score']:.3f}\")\n",
    "            print(f\"   ðŸ” Content type: {analysis['content_type']}\")\n",
    "            print(f\"   âš™ï¸  Recommended processing: {analysis['recommended_processing']}\")\n",
    "        \n",
    "        print(\"\\\\nâœ¨ Demo completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Demo failed: {str(e)}\")\n",
    "        print(\"ðŸ’¡ Make sure you have the test image file available\")\n",
    "\n",
    "# Run demo\n",
    "demo_real_world_applications()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daf5861",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ SonuÃ§ ve Gelecek AdÄ±mlar\n",
    "\n",
    "## ðŸ“ˆ GeliÅŸtirdiÄŸimiz Ä°yileÅŸtirmeler\n",
    "\n",
    "### 1. **Adaptif Token Mapping**\n",
    "- âœ… GÃ¶rÃ¼ntÃ¼ karmaÅŸÄ±klÄ±ÄŸÄ±na gÃ¶re otomatik unshuffle factor ayarÄ±\n",
    "- âœ… Attention mechanism ile Ã¶nemli token'larÄ± belirleme\n",
    "- âœ… Dinamik model kapasitesi\n",
    "\n",
    "### 2. **Multi-Scale Processing**\n",
    "- âœ… FarklÄ± resolution'larda eÅŸ zamanlÄ± iÅŸleme\n",
    "- âœ… Scale fusion attention mechanism\n",
    "- âœ… Daha zengin Ã¶zellik Ã§Ä±karÄ±mÄ±\n",
    "\n",
    "### 3. **Multimodal Capabilities**\n",
    "- âœ… Vision-Language joint processing\n",
    "- âœ… Cross-modal attention mechanisms\n",
    "- âœ… Multiple task heads (classification, captioning)\n",
    "\n",
    "### 4. **Performance Optimizations**\n",
    "- âœ… Linear attention (O(n) complexity)\n",
    "- âœ… Quantization support\n",
    "- âœ… Grouped convolutions\n",
    "- âœ… Efficient deployment pipeline\n",
    "\n",
    "## ðŸš€ Ã–nerilen Gelecek AdÄ±mlar\n",
    "\n",
    "### KÄ±sa Vadeli (1-2 hafta)\n",
    "1. **Benchmark Testing**: Standart dataset'lerde performance testi\n",
    "2. **Fine-tuning Pipeline**: Specific task'lar iÃ§in adaptasyon\n",
    "3. **Memory Optimization**: Gradient checkpointing, mixed precision\n",
    "4. **Validation**: GerÃ§ek gÃ¶rÃ¼ntÃ¼ dataset'leriyle test\n",
    "\n",
    "### Orta Vadeli (1-2 ay)\n",
    "1. **Knowledge Distillation**: BÃ¼yÃ¼k modelden bilgi transferi\n",
    "2. **Pruning Techniques**: Model compression\n",
    "3. **Advanced Attention**: Sparse attention, sliding window\n",
    "4. **Video Extension**: Temporal dimension ekleme\n",
    "\n",
    "### Uzun Vadeli (3-6 ay)\n",
    "1. **Custom CUDA Kernels**: Ultra-fast inference\n",
    "2. **Hardware-specific Optimization**: Mobile, edge devices\n",
    "3. **Research Contributions**: Academic paper writing\n",
    "4. **Open Source Release**: Community contribution\n",
    "\n",
    "## ðŸ’¡ Pratik Uygulamalar\n",
    "\n",
    "### Hemen BaÅŸlayabileceÄŸiniz Projeler:\n",
    "1. **Image Search Engine**: Token similarity ile gÃ¶rÃ¼ntÃ¼ arama\n",
    "2. **Content Moderation**: Inappropriate content detection\n",
    "3. **Medical Imaging**: X-ray, MRI analysis\n",
    "4. **Satellite Imagery**: Geographic feature detection\n",
    "5. **Fashion/E-commerce**: Product similarity matching\n",
    "\n",
    "### Gerekli Kaynaklar:\n",
    "- **Dataset**: ImageNet, COCO, custom data\n",
    "- **Compute**: GPU cluster for training\n",
    "- **Evaluation**: Standard metrics, human evaluation\n",
    "- **Deployment**: Cloud services, edge deployment\n",
    "\n",
    "## ðŸŽ‰ Ã–zet\n",
    "Bu notebook ile baÅŸlangÄ±Ã§taki basit pixel unshuffle implementasyonundan, production-ready multimodal vision system'e kadar kapsamlÄ± bir geliÅŸim yolculuÄŸu oluÅŸturduk. Her adÄ±m real-world applications'a odaklanarak pratik Ã§Ã¶zÃ¼mler sundu."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
